# Note

> For my own convenience, I will write this note in Chinese
>
> Maybe I'll translate it to English later...

## 强化学习

> Reinforcement Learning

### Q-learning

$Q$ 表

- 行为状态 $s$，列为行为 $a$
- 值为期望
- 我们希望对于每一个状态，只要选取期望最大的行为去做，就可以取得最好的结果

算法

```
R = [...](m*n 维)
探索率 = 0.8
学习率 = 0.8
折扣率 = 0.8

Q = [0](m*n 维)

while (学习未结束):
	s = 随机状态
	while (单次训练未结束):
		a = 策略(s)
		s' = 执行(a)
		Q[s,a] = bellman(s,a,s')
		s = s'
return Q
```

- $R$：奖励表，与 $Q$ 表同维度
- 探索率：$\varepsilon$
  - 为了防止陷入局部最优：在 $s_i$ 处执行 $a_1$ 后得到正奖赏，其它动作的期望都是 0，导致以后每次处于这个状态都只会采用 $a_1$
  - 因此我们决定 $\varepsilon$ 的概率随机执行动作，$(1-\varepsilon)$ 的概率按照期望执行动作
- $bellman(s,a,s')$
  - $=(1-\alpha)*Q[s,a]+\alpha*(R[s,a]+\Upsilon*max_a Q[s',a])$
  - $\alpha$：学习率
    - 学习率越大，对之前训练效果的保留就越少
  - $\Upsilon$：折扣率
    - 折扣率越大，就越关心眼前的奖励而非记忆中将要获得的奖励

## 聚类

> Clustering

**聚类的好坏不存在绝对的标准**

### 概述

- 簇/类：内部相似，不同类之间不相似
- 相似性评价标准：距离
  - 把每个样本的特征向量看做样本空间中的点
  - 样本的相似性度量就是他们之间的距离
- 算法特征
  - 类并非事先给定，而是根据距离划分**（无监督）**
    - 潜在的自然分组结构
    - 感兴趣的关系
- 关键问题
  - 特征选取
  - 距离函数

### 距离

#### 常用度量函数

- 欧氏距离
  - $d(x_i,x_j)=\sqrt{(x_i-x_j)^T(x_i-x_j)}$
  - 在二维平面中即两点间距离
- 余弦相似性
  - $s(x_i,x_j)=\frac{x_i^Tx_j}{||x_i||||x_j||}$
  - 通过计算两个向量夹角的余弦值来评判它们是否大致指向同一方向
  - 范围在 $[-1,1]$
  - $||x||=\sqrt{x·x^T}$
- 曼哈顿距离
  - $d(x_i,x_j)=||x_i-x_j||_1$
- 切比雪夫距离
  - $d(x_i,x_j)=||x_i-x_j||_\infin$
- 马氏距离
  - $d(x_i,x_j)=\sqrt{(x_i-x_j)^TM(x_i-x_j))}$
  - $M$ 为样本总体的协方差矩阵
  - 要求
    - 样本数大于样本维数
    - 样本不共低维平面
  - 与欧氏距离区别
    - 欧氏距离对于所有特征同等看待
    - 马氏距离不关心原始数据的单位，还能排除特征之间相关性的干扰

#### 距离度量空间

- $(X,d)$：$X$ 是样本集合，$d$ 是 $X$ 上的度量函数（把 $X$ 中每一对 $x,y$ 映射到一个实数）
- 四条公理
  - 非负性：$d(x,y)\geq 0$
  - 唯一性：$d(x,y)=0\leftrightarrow x=y$
  - 对称性：$d(x,y)=d(y,x)$
  - 三角不等式：$d(x,z)\leq d(x,y)+d(y,z)$

### 聚类准则

- 试探方法
  - 定义一个阈值，超过这个值则形成新类
  - 当属于多个类时，按最近邻规则
- 聚类准则函数
  - 假设有待分类样本 $x_1,...,x_n$ 被划分为 $S_1,...,S_c$ 类
  - 则有准则函数 $J=\sum_{j=1}^cK$，$K=\sum_{x\in S_{j}}||x-m_j||^2$
    - $m_j$ 代表 $S_j$ 类的均值
    - $K$ 就是 $S_j$ 类中全部样本与均值的误差平方和
  - 因此我们的目标转化为求使 $J$ 最小的聚类形式

### 聚类方法

#### 基于试探

- **按最近邻规则的简单试探**
  - 定义
    - 数据样本：$x_1,...,x_n$
    - 阈值：$T$
    - 聚类中心：$z_1,z_2...$
  - 算法
    - 随机选取初始中心，如令 $z_1=x_1$
    - 对 $x_2$，计算 $d(z_1,x_2)$
      - 若 $d>T$，$x_2$ 成为 $z_2$
      - 若 $d\leq T$，$x_2$ 分到 $z_1$
    - 对 $x_3$，计算 $d(z_1,x_3),...,d(z_k,x_3)$
      - 若 $d(z_i,x_3)$ 和 $d(z_j,x_3)$ 都小于 $T$，选择更小的那个加入
    - 重复直到所有样本分类完毕
  - 影响因素
    - 第一个聚类中心
    - 样本排列次序！
    - 阈值
    - 样本几何性质
- **最大最小距离算法**
  - 思想
    - 以最大距离作为选出聚类中心的条件
  - 算法
    - 随机选取初始中心，如令 $z_1=x_1$
    - 若 $x_2$ 离 $x_1$ 最远，则选择其作为 $z_2$
    - 计算每个剩余样本 $i$ 和每个聚类中心 $j$ 之间的距离 $D_{ij}$
      - 对于每个 $j$，选出最小的 $D_{ij}$
      - 对于选出的 $j$ 个 $D$，选择其中最大的作为 $D$
        - 若 $D>\theta||z_1-z_2||$（ $\theta$ 自己定义，如 $\frac{1}{2}$ ），则令对应的 $x_i$ 成为新的聚类中心
        - 否则，寻找聚类中心的过程结束
    - 聚类中心寻找完毕后，按最近邻规则将剩余样本点分类
      - 最后还可以在每一类中计算均值，得到更具代表性的聚类中心

#### 系统聚类法

- 思想
  - 按照距离准则逐步分类，类别由多到少
- 算法
  - 初始样本 $x_1,...,x_n$ 自成一类分为 $n$ 类 $G_1^0,...,G_n^0$
  - 计算每个 $G$ 之间的距离，得到一个 $n\times n$ 的矩阵 $D^0$
  - 现在假设前一步已求得 $D^k$
    - 则求 $D^k$ 中最小的元素，若它是 $G_i^k$ 和 $G_j^k$ 之间的距离，则将它们合并为 $G_{i}^{k+1}$
    - 计算合并后 $G_1^{k+1},...,G_i^{k+1},...$ 得到的 $D^{k+1}$
  - 重复，直到满足以下其一
    - 达到目标聚类数上限
    - $D^k$ 中最小元素超过给定阈值
- $G_i$ 和 $G_j$ 之间的距离度量
  - 所有样本点间距离的最小值
  - 所有样本点间距离的最大值
  - 所有样本点间距离的平均值

#### 动态聚类法

- 思想
  - 初始选择若干聚类中心，再按照聚类准则使样本点向中心聚集，得到初始聚类
  - 然后判断聚类是否合理，若不合理则持续修改直至合理
- **K-means**
  - 定义
    - 聚类数量：$K$
    - 初始聚类中心：$c_1,...,c_K$
  - 算法
    - 初始化聚类中心
      - 随机坐标
      - 随机样本点
    - 对每个样本点，根据最近邻规则分到对应的聚类中心
    - 重新计算聚类中心
      - 聚类中全部样本的均值
    - 如果有样本点聚类改变，则迭代；否则，完成
  - 影响因素
    - 聚类数量
    - 初始聚类中心
    - 样本几何分布
- **K-means++**
  - 思想
    - 让初始聚类中心尽可能分开
  - 算法
    - 随机选择一个样本作为初始聚类中心 $c_1$
    - 计算每个样本 $i$ 与当前已有聚类中心 $j$ 之间的距离 $D_{ij}$
    - 对每个 $i$ 选出最小的 $D_{ij}$，记为 $D(i)$
    - 计算每个 $i$ 被选为下一个聚类中心的概率 $\frac{D(i)^2}{\sum_{k=1}^nD(k)^2}$
    - 按照 **轮盘法** 选出下一个聚类中心
      - 有概率 $p_1=0.3,p_2=0.4,p_3=0.3$，代表轮盘上的三个区域
      - 系统随机一个 $0-1$ 的数，如 $0.58$，代表指针的力量
      - $0.58-0.3=0.28>0$，指针还能转，继续
      - $0.28-0.4<0$，指针转不动了，选择 $2$
    - 重复直到选出 $K$ 个中心，后续步骤同 K-means
- **ISODATA**
  - 定义
    - 初始聚类数：$K_0$
    - 类最小元素数：$K_{min}$
    - 类间最小距离：$d_{min}$
    - 类内最大方差：$\sigma_{max}$
  - 算法
    - 随机选择 $K_0$ 个样本作为 $c_1,...,c_{K_0}$
    - 对每个样本，按照最近邻规则分类
    - 判断每个类中的元素数目是否小于 $K_{min}$
      - 若小于，丢弃该类，并将其中样本重新分类
    - 对每个 $c_i$，重新计算其聚类中心 $c_i=\frac{1}{|c_i|}\sum_{x\in c_i}x$（质心）
    - 若当前 $K\leq \frac{K_0}{2}$，做分裂
      - 对于每个类别
        - 计算全部样本在每个维度下的方差
        - 挑选出最大的方差 $\sigma$
        - 若 $\sigma>\sigma_{max}$ 且该类样本数大于等于 $2N_{min}$，进行分裂
      - 将需要分裂的类别 $i$ 分成两个子类
        - $c_i^1=c_i+\sigma$
        - $c_i^2=c_i-\sigma$
        - $K=K+1$
    - 若当前 $K \geq2K_0$，做合并
      - 计算所有聚类中心两两之间的距离，用矩阵 $D$ 表示（ $D(i,i)=0$ ）
      - 若 $D(i,j)<d_{min}$，合并 $i,j$
      - 合并类的聚类中心为 $c_{new}=\frac{1}{n_i+n_j}(n_ic_i+n_jc_j)$
        - $n_i$ 为聚类 $i$ 中的元素数
    - 若达到最大迭代次数则终止，否则回到 step2
  - 与 K-means 区别
    - K-means 适合已知类别数目，ISODATA 更灵活
    - ISODATA 可以在中途改变参数，可以更好地人机交互，利用中间结果经验
    - ISODATA 需要额外指定较多参数

#### 聚类评价

- 指标
  - 聚类中心之间的距离：尽可能大
  - 聚类中的样本数目：数目较少且距离较远时可能是噪声
  - 聚类中样本的距离方差：方差过大的样本可能应该分到其它类
- 常用指标
  - **标签未知**
  - $\Omega_i$ 表示一个类，$w_i$ 表示该类的中心，$K$ 表示类的总数
  - 紧密度
    - 越小表示类内约紧密（没有考虑类间聚类效果）
    - $\overline{CP}_k=\frac{1}{|\Omega_i|}\sum_{x\in \Omega_i}||x-w_i||$
    - $\overline{CP}=\frac{1}{K}\sum_{k=1}^K\overline{CP_k}$
  - 间隔度
    - 越大表示类间约分散（没有考虑类内聚类效果）
    - $\overline{SP}=\frac{2}{K^2-K}\sum_{i=1}^K\sum_{j=i+1}^K||w_i-w_j||^2$
  - 戴维森堡丁指数/分类适确性指标
    - 越小表示类内约紧凑，类间越分散（适用于欧氏距离，不适于环状分布）
    - $DBI=\frac{1}{K}\sum_{i=1}^Kmax_{i\neq j}(\frac{\overline{C}_i+\overline{C}_j}{||w_i-w_j||_2})$
  - 邓恩指数
    - 越大表示类内约紧凑，类间越分散（适用于离散样本，不适于环状分布）
    - $DVI=\frac{d_{min}}{d_{max}}$
      - $d_{min}$ 表示 任意两个类内元素间的最短距离 的最小值
      - $d_{max}$ 表示 任意类内任意元素间最大距离 的最大值
  - **标签已知**
  - 聚类准确率（CA）；兰德指数（RI）；调整兰德指数（ARI）；互信息（MI）；归一化互信息（NMI）

