# Note

> For my own convenience, I will write this note in Chinese
>
> Maybe I'll translate it to English later...

## 强化学习

> Reinforcement Learning

### 概述

- 介于监督和无监督之间
  - 仅告知答案是否正确，不告诉学习者如何优化
  - 学习者要尝试不同的策略来挑出最好的
  - 奖赏最大化
- 智能体 $\rightarrow$ 行为 $\rightarrow$ 环境 $\rightarrow$ 状态&奖赏 $\rightarrow$ 智能体
  - 奖赏延迟：很长一段时间之后才会得到奖赏
  - 策略：在当前状态中选择行为
-  本质：奖惩和试错（Trail and Error）

### MDP

- 马尔科夫决策过程：Markov Decision Process
- 马氏性
  - 下一个决策只与当前状态和选择的行为有关
  - 与如何来到当前状态（历史）无关
- 定义
  - $S$：状态集合
  - $A$：动作集合
  - $\delta(s,a,s')$：状态 $s$ 采取动作 $a$ 转移到状态 $s'$ 的概率
  - $R(s,a)$：状态 $s$ 采取动作 $a$ 的即时奖赏
  - $\pi:S\rightarrow A$：策略，状态到动作的映射
  - 轨迹：一次完整迭代的经验（状态、动作、奖赏的一条链）
  - 返回值：将多个即时奖赏组合成单个值
    - 通常是线性组合
    - 要不要考虑早晚期的奖赏哪个更重要？
      - 折扣因子： $\gamma$
      - $G=\sum_i\gamma^iR_i$
- 问题：如何求解马尔科夫决策过程？
  - 动态规划

### 动态规划

- 前提：给定了一个 **完全已知** 的 MDP 模型
- 要做的事
  - 策略评估：给定策略 $\pi$，评估其返回值
  - 最优控制：寻找最优策略 $\pi^*$（从任一状态出发，返回值都最大）
- 值函数
  - 状态价值：$V^\pi(s)$：从状态 $s$ 出发，采用 $\pi$ 策略，所获得的返回值
  - 行动价值：$Q^\pi(s,a)$：从状态 $s$ 出发，采用 $a$ 动作和 $\pi$ 策略，所获得的返回值
  - $\pi^*$ 为最优策略当且仅当
    - $V^{\pi^*}(s)=max_\pi V^\pi(s)$
    - $V^\pi(s)=max_a Q^\pi(s,a)$
- 策略评估
  - $Bellman$ 等式
  - $V^\pi(s)=E[R(s,\pi(s))]+\gamma\sum_{s'}\delta(s,\pi(s),s')V^\pi(s')$
    - 所有值函数构成了一个公式组，需要用线性规划求解
    - 用数学期望 $E[...]$ 的原因是 $\pi(s)$ 是随机的
- 最优控制
  - $Q^\pi(s,a)=E[R(s,a)]+\gamma\sum_{s'}\delta(s,a,s')V^\pi(s')$
- 策略更新
  - 贪心
    - $\pi(s)=a_{max(Q^\pi(s,a))}$
  - $\varepsilon$ - 贪心
    - $\varepsilon$ 的概率选择其它动作
    - $1-\varepsilon$ 的概率贪心
- 迭代方法
  - 初始化 $\pi_0$
  - 策略评估得到 $V^{\pi_0}$
  - 通过最优控制得到 $\pi_1$
  - 直至收敛得到 $\pi^*$
- 问题：当 MDP 模型非已知（大多数强化学习问题都是如此）的时候，要怎么进行求解呢？
  - Monte Carlo 方法

### Monte Carlo 方法

- 基本思想：通过采样逼近真实的环境模型
  - 用多次轨迹的平均价值来逼近状态的真实价值
- 策略评估
  - Every-Visit：在每一个轨迹中，对每一次访问状态得到的价值都进行平均
    - $V(s_t)=\frac{V_{11}(s_t)+V_{12}(s_t)+V_{21}(s_t)}{3}$
  - First-Visit：在每一个轨迹中，只采用第一次访问状态得到的价值进行平均【常用这个】
    - $V(s_t)=\frac{V_{1}(s_t)+V_{2}(s_t)}{2}$
  - 补充：$V(s_t)$ 怎么算？
    - $G(s_t)=\gamma^tR_t+...+\gamma^nR_n$
    - **价值** 其实就是在到达这个状态之后你还能期望获得的奖赏
  - 想要在每一个轨迹结束时都更新 $V(s)$ 怎么办？
    - $N_{new}(s_t)=N(s_t)+1$
    - $V_{new}(s_t)=V(s_t)+\frac{1}{N_{new}(s_t)}(G(s_t)-V(s_t))$
- 迭代方法
  - 只有策略评估与动态规划不同，不采用 $Bellman$ 而是 $MC$
- 问题：能不能每走一步都可以更新一次 $V(s_t)$，而不用等到轨迹结束？
  - 时序差分法

### 时序差分法

- 也称 $TD$ 法
- $V(s_t)=V(s_t)+\alpha[R_{t}+\gamma V(s_{t+1})-V(s_t)]$
  - 由于没有走完，不能清楚地知道 $\frac{1}{N(s_t)}$，所以用一个 $\alpha\in (0,1]$ 代替
  - $V(s_{t+1})$：下一个状态的预估价值（上一次轨迹保留下来的值）
  - $\gamma$：折扣因子
- 区别
  - $MC$ 是 $Sampling$，只根据经验进行更新，所以要等到轨迹走完
  - 时序差分是 $Boostraping$，根据估计进行更新
- $TD(n)$
  - 上述的 $TD$ 法其实是 $TD(0)$
  - 想要看见更远的未来
  - $G^{(n)}_t=R_t+\gamma R_{t+1}+...+\gamma^{n-1}R_{t+n-1}+\gamma^n V(s_{t+n})$
  - 当 $n$ 越来越大，趋于完整轨迹时，等价于 $MD$
- $TD(\lambda)$
  - 那么怎样选择 $n$ 才能获得最优的效果呢？
    - 不选择，以所有步数作参考
    - 但对于每一个 $n$ 定义一个合适的权重 $(1-\lambda)\lambda^{n-1}$
  - 引入 $\lambda\in[0,1]$，定义 $G^\lambda_t=(1-\lambda)\sum_{n=1}^\infin\lambda^{n-1}G^{(n)}_t$ 为收获
  - 这样，迭代公式就转变为 $V(s_t)=V(s_t)+\alpha[G^\lambda_t-V(s_t)]$
  - 当 $\lambda=0$ 时为 $TD$；当 $\lambda=1$ 时为 $MC$
  - 算法
    - 初始化全部 $V(s),e(s)=0$
      - $e(s)$ 为 **效用**，代表该状态对后续状态的影响
    - 对每一个 episode
      - 初始化 $s$
      - 对 episode 中的每一步
        - 根据 $\varepsilon-greedy$ 选择 $a$
        - 执行 $a$，获得 $r$ 和 $s'$
        - $\Delta=r+\gamma V(s')-V(s)$
        - $e(s)=e(s)+1$
        - 对每一个 $s$
          - $V(s)=V(s)+\alpha*\Delta*e(s)$
          - $e(s)=\gamma*\lambda*e(s)$
        - $s\leftarrow s'$
      - 直至 $s$ 为终止状态

### Q-learning

$Q$ 表

- 行为状态 $s$，列为行为 $a$
- 值为期望
- 我们希望对于每一个状态，只要选取期望最大的行为去做，就可以取得最好的结果

算法

```
R = [...](m*n 维)
探索率 = 0.8
学习率 = 0.8
折扣率 = 0.8

Q = [0](m*n 维)

while (学习未结束):
	s = 随机状态
	while (单次训练未结束):
		a = 策略(s)
		s' = 执行(a)
		Q[s,a] = bellman(s,a,s')
		s = s'
return Q
```

- $R$：奖励表，与 $Q$ 表同维度
- 探索率：$\varepsilon$
  - 为了防止陷入局部最优：在 $s_i$ 处执行 $a_1$ 后得到正奖赏，其它动作的期望都是 0，导致以后每次处于这个状态都只会采用 $a_1$
  - 因此我们决定 $\varepsilon$ 的概率随机执行动作，$(1-\varepsilon)$ 的概率按照期望执行动作
- $bellman(s,a,s')$
  - $=(1-\alpha)*Q[s,a]+\alpha*(R[s,a]+\Upsilon*max_a Q[s',a])$
  - $\alpha$：学习率
    - 学习率越大，对之前训练效果的保留就越少
  - $\Upsilon$：折扣率
    - 折扣率越大，就越关心眼前的奖励而非记忆中将要获得的奖励

## 聚类

> Clustering

**聚类的好坏不存在绝对的标准**

### 概述

- 簇/类：内部相似，不同类之间不相似
- 相似性评价标准：距离
  - 把每个样本的特征向量看做样本空间中的点
  - 样本的相似性度量就是他们之间的距离
- 算法特征
  - 类并非事先给定，而是根据距离划分**（无监督）**
    - 潜在的自然分组结构
    - 感兴趣的关系
- 关键问题
  - 特征选取
  - 距离函数

### 距离

#### 常用度量函数

- 欧氏距离
  - $d(x_i,x_j)=\sqrt{(x_i-x_j)^T(x_i-x_j)}$
  - 在二维平面中即两点间距离
- 余弦相似性
  - $s(x_i,x_j)=\frac{x_i^Tx_j}{||x_i||||x_j||}$
  - 通过计算两个向量夹角的余弦值来评判它们是否大致指向同一方向
  - 范围在 $[-1,1]$
  - $||x||=\sqrt{x·x^T}$
- 曼哈顿距离
  - $d(x_i,x_j)=||x_i-x_j||_1$
- 切比雪夫距离
  - $d(x_i,x_j)=||x_i-x_j||_\infin$
- 马氏距离
  - $d(x_i,x_j)=\sqrt{(x_i-x_j)^TM(x_i-x_j))}$
  - $M$ 为样本总体的协方差矩阵
  - 要求
    - 样本数大于样本维数
    - 样本不共低维平面
  - 与欧氏距离区别
    - 欧氏距离对于所有特征同等看待
    - 马氏距离不关心原始数据的单位，还能排除特征之间相关性的干扰

#### 距离度量空间

- $(X,d)$：$X$ 是样本集合，$d$ 是 $X$ 上的度量函数（把 $X$ 中每一对 $x,y$ 映射到一个实数）
- 四条公理
  - 非负性：$d(x,y)\geq 0$
  - 唯一性：$d(x,y)=0\leftrightarrow x=y$
  - 对称性：$d(x,y)=d(y,x)$
  - 三角不等式：$d(x,z)\leq d(x,y)+d(y,z)$

### 聚类准则

- 试探方法
  - 定义一个阈值，超过这个值则形成新类
  - 当属于多个类时，按最近邻规则
- 聚类准则函数
  - 假设有待分类样本 $x_1,...,x_n$ 被划分为 $S_1,...,S_c$ 类
  - 则有准则函数 $J=\sum_{j=1}^cK$，$K=\sum_{x\in S_{j}}||x-m_j||^2$
    - $m_j$ 代表 $S_j$ 类的均值
    - $K$ 就是 $S_j$ 类中全部样本与均值的误差平方和
  - 因此我们的目标转化为求使 $J$ 最小的聚类形式

### 聚类方法

#### 基于试探

- **按最近邻规则的简单试探**
  - 定义
    - 数据样本：$x_1,...,x_n$
    - 阈值：$T$
    - 聚类中心：$z_1,z_2...$
  - 算法
    - 随机选取初始中心，如令 $z_1=x_1$
    - 对 $x_2$，计算 $d(z_1,x_2)$
      - 若 $d>T$，$x_2$ 成为 $z_2$
      - 若 $d\leq T$，$x_2$ 分到 $z_1$
    - 对 $x_3$，计算 $d(z_1,x_3),...,d(z_k,x_3)$
      - 若 $d(z_i,x_3)$ 和 $d(z_j,x_3)$ 都小于 $T$，选择更小的那个加入
    - 重复直到所有样本分类完毕
  - 影响因素
    - 第一个聚类中心
    - 样本排列次序！
    - 阈值
    - 样本几何性质
- **最大最小距离算法**
  - 思想
    - 以最大距离作为选出聚类中心的条件
  - 算法
    - 随机选取初始中心，如令 $z_1=x_1$
    - 若 $x_2$ 离 $x_1$ 最远，则选择其作为 $z_2$
    - 计算每个剩余样本 $i$ 和每个聚类中心 $j$ 之间的距离 $D_{ij}$
      - 对于每个 $j$，选出最小的 $D_{ij}$
      - 对于选出的 $j$ 个 $D$，选择其中最大的作为 $D$
        - 若 $D>\theta||z_1-z_2||$（ $\theta$ 自己定义，如 $\frac{1}{2}$ ），则令对应的 $x_i$ 成为新的聚类中心
        - 否则，寻找聚类中心的过程结束
    - 聚类中心寻找完毕后，按最近邻规则将剩余样本点分类
      - 最后还可以在每一类中计算均值，得到更具代表性的聚类中心

#### 系统聚类法

- 思想
  - 按照距离准则逐步分类，类别由多到少
- 算法
  - 初始样本 $x_1,...,x_n$ 自成一类分为 $n$ 类 $G_1^0,...,G_n^0$
  - 计算每个 $G$ 之间的距离，得到一个 $n\times n$ 的矩阵 $D^0$
  - 现在假设前一步已求得 $D^k$
    - 则求 $D^k$ 中最小的元素，若它是 $G_i^k$ 和 $G_j^k$ 之间的距离，则将它们合并为 $G_{i}^{k+1}$
    - 计算合并后 $G_1^{k+1},...,G_i^{k+1},...$ 得到的 $D^{k+1}$
  - 重复，直到满足以下其一
    - 达到目标聚类数上限
    - $D^k$ 中最小元素超过给定阈值
- $G_i$ 和 $G_j$ 之间的距离度量
  - 所有样本点间距离的最小值
  - 所有样本点间距离的最大值
  - 所有样本点间距离的平均值

#### 动态聚类法

- 思想
  - 初始选择若干聚类中心，再按照聚类准则使样本点向中心聚集，得到初始聚类
  - 然后判断聚类是否合理，若不合理则持续修改直至合理
- **K-means**
  - 定义
    - 聚类数量：$K$
    - 初始聚类中心：$c_1,...,c_K$
  - 算法
    - 初始化聚类中心
      - 随机坐标
      - 随机样本点
    - 对每个样本点，根据最近邻规则分到对应的聚类中心
    - 重新计算聚类中心
      - 聚类中全部样本的均值
    - 如果有样本点聚类改变，则迭代；否则，完成
  - 影响因素
    - 聚类数量
    - 初始聚类中心
    - 样本几何分布
- **K-means++**
  - 思想
    - 让初始聚类中心尽可能分开
  - 算法
    - 随机选择一个样本作为初始聚类中心 $c_1$
    - 计算每个样本 $i$ 与当前已有聚类中心 $j$ 之间的距离 $D_{ij}$
    - 对每个 $i$ 选出最小的 $D_{ij}$，记为 $D(i)$
    - 计算每个 $i$ 被选为下一个聚类中心的概率 $\frac{D(i)^2}{\sum_{k=1}^nD(k)^2}$
    - 按照 **轮盘法** 选出下一个聚类中心
      - 有概率 $p_1=0.3,p_2=0.4,p_3=0.3$，代表轮盘上的三个区域
      - 系统随机一个 $0-1$ 的数，如 $0.58$，代表指针的力量
      - $0.58-0.3=0.28>0$，指针还能转，继续
      - $0.28-0.4<0$，指针转不动了，选择 $2$
    - 重复直到选出 $K$ 个中心，后续步骤同 K-means
- **ISODATA**
  - 定义
    - 初始聚类数：$K_0$
    - 类最小元素数：$K_{min}$
    - 类间最小距离：$d_{min}$
    - 类内最大方差：$\sigma_{max}$
  - 算法
    - 随机选择 $K_0$ 个样本作为 $c_1,...,c_{K_0}$
    - 对每个样本，按照最近邻规则分类
    - 判断每个类中的元素数目是否小于 $K_{min}$
      - 若小于，丢弃该类，并将其中样本重新分类
    - 对每个 $c_i$，重新计算其聚类中心 $c_i=\frac{1}{|c_i|}\sum_{x\in c_i}x$（质心）
    - 若当前 $K\leq \frac{K_0}{2}$，做分裂
      - 对于每个类别
        - 计算全部样本在每个维度下的方差
        - 挑选出最大的方差 $\sigma$
        - 若 $\sigma>\sigma_{max}$ 且该类样本数大于等于 $2N_{min}$，进行分裂
      - 将需要分裂的类别 $i$ 分成两个子类
        - $c_i^1=c_i+\sigma$
        - $c_i^2=c_i-\sigma$
        - $K=K+1$
    - 若当前 $K \geq2K_0$，做合并
      - 计算所有聚类中心两两之间的距离，用矩阵 $D$ 表示（ $D(i,i)=0$ ）
      - 若 $D(i,j)<d_{min}$，合并 $i,j$
      - 合并类的聚类中心为 $c_{new}=\frac{1}{n_i+n_j}(n_ic_i+n_jc_j)$
        - $n_i$ 为聚类 $i$ 中的元素数
    - 若达到最大迭代次数则终止，否则回到 step2
  - 与 K-means 区别
    - K-means 适合已知类别数目，ISODATA 更灵活
    - ISODATA 可以在中途改变参数，可以更好地人机交互，利用中间结果经验
    - ISODATA 需要额外指定较多参数

#### 聚类评价

- 指标
  - 聚类中心之间的距离：尽可能大
  - 聚类中的样本数目：数目较少且距离较远时可能是噪声
  - 聚类中样本的距离方差：方差过大的样本可能应该分到其它类
- 常用指标
  - **标签未知**
  - $\Omega_i$ 表示一个类，$w_i$ 表示该类的中心，$K$ 表示类的总数
  - 紧密度
    - 越小表示类内约紧密（没有考虑类间聚类效果）
    - $\overline{CP}_k=\frac{1}{|\Omega_i|}\sum_{x\in \Omega_i}||x-w_i||$
    - $\overline{CP}=\frac{1}{K}\sum_{k=1}^K\overline{CP_k}$
  - 间隔度
    - 越大表示类间约分散（没有考虑类内聚类效果）
    - $\overline{SP}=\frac{2}{K^2-K}\sum_{i=1}^K\sum_{j=i+1}^K||w_i-w_j||^2$
  - 戴维森堡丁指数/分类适确性指标
    - 越小表示类内约紧凑，类间越分散（适用于欧氏距离，不适于环状分布）
    - $DBI=\frac{1}{K}\sum_{i=1}^Kmax_{i\neq j}(\frac{\overline{C}_i+\overline{C}_j}{||w_i-w_j||_2})$
  - 邓恩指数
    - 越大表示类内约紧凑，类间越分散（适用于离散样本，不适于环状分布）
    - $DVI=\frac{d_{min}}{d_{max}}$
      - $d_{min}$ 表示 任意两个类内元素间的最短距离 的最小值
      - $d_{max}$ 表示 任意类内任意元素间最大距离 的最大值
  - **标签已知**
  - 聚类准确率（CA）；兰德指数（RI）；调整兰德指数（ARI）；互信息（MI）；归一化互信息（NMI）

